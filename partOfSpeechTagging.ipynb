{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSCI 544 HW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/train.json') as f:\n",
    "    train_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Vocabulary Creation (20 points)\n",
    "Ran through the dataset and filter out words that have amount below threshold (make them \\<unk\\>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  23183\n",
      "Replace amount:  20011\n"
     ]
    }
   ],
   "source": [
    "# Task 1: Vocabulary Creation\n",
    "dic = defaultdict(int)\n",
    "threshold = 2\n",
    "for i in range(len(train_data)):\n",
    "    cur_sentence = train_data[i]['sentence']\n",
    "    for w in cur_sentence:\n",
    "        dic[w] += 1\n",
    "sorted_list = sorted(dic.items(), key = lambda x:x[1], reverse = True)\n",
    "unknown_count = 0\n",
    "replace_count = 0\n",
    "for k, v in sorted_list:\n",
    "    if v < threshold:\n",
    "        replace_count += 1\n",
    "        unknown_count += v\n",
    "vocab = {'<unk>': (0, unknown_count)}\n",
    "for i, cur in enumerate(sorted_list):\n",
    "    cur_w = cur[0]\n",
    "    cur_v = cur[1]\n",
    "    if cur_v < threshold: continue\n",
    "    vocab[cur_w] =  (i + 1, cur_v)\n",
    "    \n",
    "print(\"Vocabulary size: \", len(vocab))\n",
    "print(\"Replace amount: \", replace_count)\n",
    "\n",
    "\n",
    "with open('vocab.txt', 'w') as f_out:\n",
    "    for word, item in vocab.items():\n",
    "        index = item[0]\n",
    "        count = item[1]\n",
    "        f_out.write(str(word)+'\\t'+str(index)+'\\t'+str(count)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What threshold value did you choose for identifying unknown words for replacement? </br>\n",
    "Ans: 2 </br>\n",
    "Q: What is the overall size of your vocabulary, and how many times does the special token ”< unk >” occur following the replacement process? </br>\n",
    "Ans: Vocabulary overall size -> 23183, replacement occur -> 20011 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Model Learning (20 points)\n",
    "Calculate the transition and emission probs for later implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition parameter size:  1392\n",
      "Emission parameter size:  30303\n"
     ]
    }
   ],
   "source": [
    "#Task 2: Model Learning\n",
    "transition_data_count = defaultdict(int)\n",
    "state_count = defaultdict(int)\n",
    "emission_data_count = defaultdict(int)\n",
    "sentence_count = len(train_data)\n",
    "tag_list = []\n",
    "\n",
    "for row in train_data:\n",
    "    cur_s = row['sentence']\n",
    "    cur_labels = row['labels']\n",
    "    pre = '<s>'\n",
    "    end_index = len(cur_s) - 1\n",
    "    for i, pair in enumerate(zip(cur_s, cur_labels)):\n",
    "        word = pair[0]\n",
    "        if word not in vocab:\n",
    "            word = '<unk>'\n",
    "        label = pair[1]\n",
    "        if label not in tag_list:\n",
    "            tag_list.append(label)\n",
    "        transition_data_count[(pre, label)] += 1\n",
    "        state_count[pre] += 1\n",
    "        emission_data_count[(label, word)] += 1\n",
    "        pre = label\n",
    "\n",
    "tran_dict = defaultdict(float)\n",
    "emission_dict = defaultdict(float)\n",
    "\n",
    "for key, val in transition_data_count.items():\n",
    "    state = key[0]\n",
    "    next_state = key[1]\n",
    "    tran_dict[str(key)] = val / state_count[state]\n",
    "\n",
    "for key, val in emission_data_count.items():\n",
    "    state = key[0]\n",
    "    word = key[1]\n",
    "    emission_dict[str(key)] = val / state_count[state]\n",
    "\n",
    "print(\"Transition parameter size: \", len(tran_dict))\n",
    "print(\"Emission parameter size: \", len(emission_dict))\n",
    "hmm_file = {' transition': tran_dict, 'emission': emission_dict}\n",
    "with open(\"hmm.json\", \"w\") as f_out:\n",
    "    json.dump(hmm_file, f_out, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: How many transition and emission parameters in your HMM? </br>\n",
    "Ans: 1392 for transition and 30303 for emission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Greedy Decoding with HMM (30 points)\n",
    "Using Greedy Decoding algorithm to implement HMM, and output results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 3: Greedy Decoding with HMM\n",
    "def greedy_decoding(sentence):\n",
    "    tag_predictions = []\n",
    "    pre = '<s>'\n",
    "    for w in sentence:\n",
    "        if w not in vocab:\n",
    "            w = '<unk>'\n",
    "        cur_tag = None\n",
    "        max_prob = float('-inf')\n",
    "        for tag in tag_list:\n",
    "            cur_prob = tran_dict[str((pre, tag))] * emission_dict[str((tag, w))]\n",
    "            if cur_prob > max_prob:\n",
    "                max_prob = cur_prob\n",
    "                cur_tag = tag\n",
    "        pre = cur_tag\n",
    "        tag_predictions.append(cur_tag)\n",
    "    return tag_predictions\n",
    "\n",
    "with open('./data/test.json') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "test_result = []\n",
    "for i, row in enumerate(test_data):\n",
    "    cur_sentence = row['sentence']\n",
    "    tag_predictions = greedy_decoding(cur_sentence)\n",
    "    test_result.append({'index': i, 'sentence': cur_sentence, 'labels': tag_predictions})\n",
    "    \n",
    "with open(\"greedy.json\", \"w\") as f_out:\n",
    "    json.dump(test_result, f_out, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Decoding with HMM's accuracy on the dev data:  0.9350221601602817\n"
     ]
    }
   ],
   "source": [
    "def acc_result(predictions, ground_truth):\n",
    "    total = 0\n",
    "    match = 0\n",
    "    for pred, truth in zip(predictions, ground_truth):\n",
    "        for a, b in zip(pred, truth):\n",
    "            total += 1\n",
    "            if a == b:\n",
    "                match += 1\n",
    "    return float(match) / float(total)\n",
    "\n",
    "with open('./data/dev.json') as f:\n",
    "    dev_data = json.load(f)\n",
    "\n",
    "predictions = []\n",
    "ground_truth = []\n",
    "for row in dev_data:\n",
    "    cur_sentence = row['sentence']\n",
    "    cur_labels = row['labels']\n",
    "    ground_truth.append(cur_labels)\n",
    "    predictions.append(greedy_decoding(cur_sentence))\n",
    "\n",
    "print(\"Greedy Decoding with HMM's accuracy on the dev data: \", acc_result(predictions, ground_truth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What is the accuracy on the dev data? </br>\n",
    "Ans: 93.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Viterbi Decoding with HMM (30 Points)\n",
    "Using Viterbi Decoding algorithm to implement HMM, and output results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Decoding with HMM\n",
    "def viterbi_decoding(sentence):\n",
    "    tag_predictions = []\n",
    "    n = len(sentence)\n",
    "    m = len(tag_list)\n",
    "    v_table = [[0 for _ in range(m)] for __ in range(n)]\n",
    "    from_table = [[-1 for _ in range(m)] for __ in range(n)]\n",
    "    for i, w in enumerate(sentence):\n",
    "        if w not in vocab:\n",
    "            w = '<unk>'\n",
    "        if i == 0:\n",
    "            pre = '<s>'\n",
    "            for j, tag in enumerate(tag_list):\n",
    "                v_table[i][j] = tran_dict[str((pre, tag))] * emission_dict[str((tag, w))]\n",
    "        else:\n",
    "            for j, tag in enumerate(tag_list):\n",
    "                for k, tag_pre in enumerate(tag_list):\n",
    "                    cur_prob = v_table[i - 1][k] * tran_dict[str((tag_pre, tag))] * emission_dict[str((tag, w))]\n",
    "                    if cur_prob > v_table[i][j]:\n",
    "                        v_table[i][j] = cur_prob\n",
    "                        from_table[i][j] = k\n",
    "    \n",
    "    pre_tag_index = -1\n",
    "    cur_tag = None\n",
    "    last_prob = float('-inf')\n",
    "    for tag_index in range(m):\n",
    "        if v_table[n-1][tag_index] > last_prob:\n",
    "            last_prob = v_table[-1][tag_index]\n",
    "            cur_tag = tag_list[tag_index]\n",
    "            pre_tag_index = from_table[-1][tag_index]\n",
    "    tag_predictions.append(cur_tag)\n",
    "    \n",
    "    for i in range(n - 2, -1, -1):\n",
    "        cur_tag = tag_list[pre_tag_index]\n",
    "        pre_tag_index = from_table[i][pre_tag_index]\n",
    "        tag_predictions.append(cur_tag)\n",
    "    tag_predictions.reverse()\n",
    "    \n",
    "    return tag_predictions\n",
    "\n",
    "test_result = []\n",
    "for i, row in enumerate(test_data):\n",
    "    cur_sentence = row['sentence']\n",
    "    tag_predictions = viterbi_decoding(cur_sentence)\n",
    "    test_result.append({'index': i, 'sentence': cur_sentence, 'labels': tag_predictions})\n",
    "    \n",
    "with open(\"viterbi.json\", \"w\") as f_out:\n",
    "    json.dump(test_result, f_out, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi Decoding with HMM's accuracy on the dev data:  0.9473013174670634\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "ground_truth = []\n",
    "for row in dev_data:\n",
    "    cur_sentence = row['sentence']\n",
    "    cur_labels = row['labels']\n",
    "    ground_truth.append(cur_labels)\n",
    "    predictions.append(viterbi_decoding(cur_sentence))\n",
    "\n",
    "print(\"Viterbi Decoding with HMM's accuracy on the dev data: \", acc_result(predictions, ground_truth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What is the accuracy on the dev data? </br>\n",
    "Ans: 94.7%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS544",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
